---
title: "Tarea 1: Exploración y Visualización de Datos"
author: "Felipe Bravo, Hernán Sarmiento, Aymé Arango, Alison Fernandez, Cinthia Mabel Sanchez, Juan Pablo Silva"
date: "Septiembre 2020"
output:
  html_document:
    number_sections: yes
    theme: spacelab
    toc: yes
  pdf_document:
    toc: yes
---
# Declaración de compromiso ético
Nosotros Nicolás Herrera, Sebastián Urbina y Samuel Sánchez, declaramos que realizamos de manera grupal los pasos de la presente actividad. También declaramos no incurrir en copia, ni compartir nuestras respuestas con otras personas ni con otros grupos. Por lo que, ratificamos que las respuestas son de nuestra propia confección y reflejan nuestro propio conocimiento.

# Instrucciones

1. Trabajen en equipos de dos o tres personas. Salvo excepciones, no se corregirá entregas con menos de dos integrantes.

2. Modifique este archivo `.Rmd` agregando sus respuestas donde corresponda.

3. Para cada pregunta, cuando corresponda, **incluya el código fuente que utilizó para llegar a su respuesta**.

4. El formato de entrega para esta actividad es un archivo html. **Genere un archivo HTML usando RStudio** y súbalo a U-Cursos.
   Basta con que uno de los integrantes haga la entrega. Si ambos hacen una entrega en U-Cursos, se revisará cualquiera de éstas.

# Tarea 
La primera parte de esta actividad son preguntas teóricas que avanzaron en las clases del curso de Minería de datos.

## Teoría

*1. ¿Cuál es el objetivo de Minería de datos y qué la diferencia de Machine Learning? De un ejemplo.*

**Respuesta:** 
El objetivo de la Minería de Datos principalmente es obtener conocimiento a partir de grande volumenes de datos, de forma automática, no manual, pues nos tomaría años procesar tanta informacion. Asimismo, con esta información agilizar y mejorar la toma de desiciones.

Con respecto a la diferencia de Minería de Datos con Machine Learning, el ML se encarga de generar predicciones a partir de datos o específicamente conjuntos de entrenamientos, tecnicamente se entrenan algoritmos en base a grandes cantidades de datos para predecir compartamiento similar a humanos, mientras que la minería de datos nos permite obtener conclusiones o información útil a través de los datos, como puede ser patrones de comportamiento y relaciones entre variables, etc. A modo de ejemplo, podemos utilizar un algoritmo en base a Machile Learning para *predecir* si mañana va a llover, en contraste a Minería de Datos podemos obtener alguna información interesante sobre el clima, como por ejemplo que este año ha llovido más con respecto a años anterior, que el agua caída en cierta región x ha aumentado en los últimos años, etc.


*2. Describa y compare los siguientes métodos usados en Minería de datos: clasificación vs. clustering.*

**Respuesta:**
Clasificación: Corresponde a un tipo de método predictivo o de aprendizaje supervisado, que utiliza variables para predecedir valores futuros o otras variables desconocidas. Se necesita un set de entrenamiento con las respectivas variables y la clase para cada instancia, de tal forma que si tenemos datos de imágenes de gatos y queremos crear un clasificador de imágenes de gatos(si es gato o no) el set de entrenamiento debe estar previamente etiquetado o las clases definidas, de esta forma el modelo aprende y es posible que pueda identificar/clasificar una nueva imagen, prediciendo si es gato o no.

Clustering: Corresponde a un tipo de método descriptivo o de aprendizaje no supervisado, que permite encontrar patrones en ciertos conjuntos de datos, agrupar datos automaticamente por carácteristicas comunes encontradas por el algoritmo, etc. El objetivo del algoritmo es encontrar, si es que existen, patrones en base a los atributos de los datos.

Una de las principales diferencias de estos métodos, es que clasificación requiere datos con su respectiva clase o etiqueta, es por esto además que se considera como un tipo de método supervisado, en otras palabras, debemos entrenar nuestro modelo diciéndole al algoritmo a qué clase o categoría corresponde cada instancia, es esta forma aprende el algoritmo y es capaz de realizar predicciónes en base a nuevos datos. En cambio, clustering se basa únicamente en los atributos de los datos y no necesita la clase de estos. Asimismo, el objetivo de clasificación es "predecir" en cambio clustering busca "encontrar patrones".

*3. ¿Qué desafios existen en Minería de datos?*

**Respuesta:**
Los desafios se dividen en distintos conceptos:

- Escalabilidad: Se refiere principalmente a que los modelos funcionen a gran escala, es decir, con grandes cantidades de datos.

- Dimensionalidad: Se refiere a encontrar metodos que sean eficientes bajo diferentes grados de dimensionalidad, por ejemplo, puede ocurrer que se tiene un dataset con miles de características, sin embargo no se encuentran patrones en los datos, lo cual puede ser muy costoso para la máquina. Asimismo, se tiene el otro extremo, en datos con pocas características es muy dificil describir el comportamiento por falta de información. 

- Datos complejos y heterógenos: Hace alusión a la estructura de los datos y a la complejidad de asociar datos heterógenos, lo que se traduce en dificultad para encontrar patrones.

- Calidad de los datos: Tiene que ver con la forma en que están ordenados los datos, pueden haber datos incompletos, datos desbalanceados, datos mal separados, etc. La calidad es muy importante y se gasta bastante tiempo en limpiar los datos porque no todos los datos están ordenados en tablas y bien estructurados.

- Distribución de datos y propiedad: El donde y como se transfiere la información, donde se almacena y quien es el real propietario de esta.

- Privacidad: Lo cual se refiere la privacidad de nuestros datos, que tan públicos son estos y como pueden vulnerar nuestros derechos.

- Streaming: Constantemente se están generado datos no estructurados, como imágenes, videos, streaming los cuales son muy complejos de analizar.

Bajo todos los pilares mencionados anteriormente repercuten los desafios de la minería de datos, es imprescindible ir mejorando los modelos y procesos de adquisición y manejo de información.

*4. Respecto a los tipos de atributo, ¿cuál es la diferencia entre razón e intervalo?*

**Respuesta:**
Ambos definen datos cuantitativos (medidas estimables), se diferencian principalmente en que los datos de tipo intervalo no poseen un cero absoluto, mientras que lo de razon si. Es por esto que los datos de tipo intervalo no pueden ser operados con la multiplicacion ni con la division (su cero es arbitrario). Algunos ejemplos de datos intervalo son las fechas o temperaturas en °C o °F, y de datos de tipo razon el largo, la hora o la temperatura en K (la escala Kelvin si tiene una 0 absoluto).


*5. ¿Qué factores que ocasionan errores en el análisis de datos deben ser considerados para la limpieza de un set de datos?*

**Respuesta:**
A continuación, se describen distintos factores que se deben considerar en el momento de realizar una limpieza de un set de datos:

- Ruido: Es una componente aleatoria que se le agrega a los datos, se pueden añadir de forma intencional o no. Por ejemplo, puede haber un factor externo que podría modificar los datos medidos, ya sean espaciales o temporales, lo cual produce ruido en los valores tomados.

- Outliers: Corresponden a valores que son considerablemente diferente a la mayoría de los datos de un conjunto, es decir, que tienen una característica diferente a la tendencia que presentan los datos.

- Valores faltantes: Estos valores pueden ocurrir porque los datos no pudieron ser recolectados, por ejemplo, porque son datos confidenciales o los sujetos de la muestra no los quisieron proporcionar. Además, puede ocurrir que no todos los atributos de un conjunto de datos son aplicables a todos los sujetos de una muestra, lo cual generaría datos faltantes.

- Valores inconsistentes: Pueden ser valores mal ingresados por error humano, generando incosistencia en los datos, los cuales pueden generar ruido, ser un outlier o ser simplemente un valor mal ingresado por error.

- Datos duplicados: Puede ocurrir que en un mismo set de datos existan filas que contengan los mismos datos, los cuáles no aportan mayor información y pueden ser removidos dado que pueden generar problemas cuando se esté analizando o describiendo el set de datos. 


*6. ¿Qué es el análisis exploratorio de datos o EDA?*

**Respuesta:**
Es un conjunto de tecnicas definidas por Jhon W. Tukey, utilizadas para analizar y comprender de manera rapida la naturaleza de un dataset. Se basa principalmente en 2 criterios: Las estadisticas de resumen y la visualizacion de datos.
Las estadisticas de resumen son indicadores que explican propiedades de los datos (dada la gran cantidad de instancias de un dataset, para un ser humano es imposible entender lo que esta pasando solo mirando los datos), tales como: frecuencias, medidas de tendencia central, dispersion, etc.
La visualizacion de datos es la representacion grafica de un dataset, que al utilizar elementos visuales como cuadros, graficos y mapas, permite identificar las caracteristicas y relaciones entre elementos (le permite a las persona reconocer patrones o tendencias en base a su criterio o expertiz que tengan en el dominio particular).


*7. Describa las medidas de tendencia central: media y mediana. Exponga la diferencia entre ambas.*

**Respuesta:**
La media corresponde al promedio aritmético entre los valores de un conjunto numérico, este valor se obtiene mediante la suma de todos los valores del conjunto a analizar dividido por el largo de el conjunto seleccionado. Por otro lado, la mediana corresponde al valor que se encuentra en la mitad de un conjunto de datos ordenados, es decir, la mitad de los valores del conjunto de datos es mayor o menor que la mediana del conjunto. La diferencia entre la mediana y la media es que esta primera variable está relacionada con la posición que tiene el valor en un set de valores, por otro lado, la media es una forma simple de decir a qué se parecen en promedio los datos del conjunto.


*8. ¿Qué es una matriz de correlación y para qué sirve?*

**Respuesta:**
Una matriz de correlación permite visualizar qué tan relacionadas están relacionadas las varibales de un conjunto de estas. Estas matrices se pueden representar de diferentes formas, desde su forma más simple, una matriz cuadrada con diagonal igual a 1, hasta ser representadas mediantes formas más gráficas, utilizando colores o formas que indican el grado de relación entre una variable y otra. Usualmente, los valores de una matriz de correlación fluctúan entre 0 y 1, en donde, mientras más cercano a uno sea el valor asociado a las dos variables analizadas, más correlacionadas estarán estas. Además, en general para el cálculo de los coeficientes de correlación de la matriz se utiliza el método de pearson. Esta matriz de correlación además de ser útil para ver el nivel de relación entre variables, puede ser utilizada para disernir entre qué variables escoger para un modelo de regresión lineal o de regresión logística.


*9. Explique cómo se construye un Boxplot y exponga cómo se identifican los valores atípicos u outliers en este tipo de gráfico.*

**Respuesta:**
Un boxplot se construye a partir de los percentiles: 
  - Contiene un rectangulo construido en base al 1er y 3er Cuartil (Percentiles 25 y 75), de altura igual a IQR.
  - Al rectangulo lo divide horizontalmente una linea, correspondiente a la mediana (Percentil 50 o Q2)
  - A partir de cada extremo del rectangulo (superior e inferior) nace una recta o brazo de un determinado largo: El brazo inferior es de largo Li= Q1 - 1.5IQR, y el superior Ls= Q3 + 1.5IQR.
  - Se definen los valores mas extremos que el largo de cada brazo como atipicos u outliers.
El largo de los brazos y el criterio para identificar outliers se basa en el comportamiento de una distribucion normal, como tal si la mediana no esta en el centro del rectangulo, su distribucion no es simetrica.

* Qi: i-esimo Cuartil
* IQR (rango inter-cuartil): Q3-Q1



## Práctica 

En esta parte de la actividad se trabajará con los datos del Proceso Constituyente 2016-2017 publicados en el Portal de Datos Abiertos del Gobierno de Chile, para mayor información pueden ingresar al siguiente link: https://datos.gob.cl/dataset/proceso-constituyente-abierto-a-la-ciudadania. Los datos corresponden a las actas de los Encuentros Locales Autoconvocados (ELAs), en cada cual, un grupo de personas se reune a discutir distintos conceptos como por ejemplo: salud, educación, seguridad, etc.

Los datos con los que trabajarán consisten en la cantidad de veces que cada concepto constitucional fue mencionado por cada localidad de Chile. 

Para cargar los datos, use:

```{r}
data_tf <- read.csv("http://dcc.uchile.cl/~hsarmien/mineria/datasets/actas.txt", encoding = "UTF-8", as.is = FALSE, header = T)
```

**Por cada pregunta adjunte el código R que utilizó para llegar a la respuesta. Respuestas sin código no recibirán puntaje**
### Exploración básica

1. ¿Cuáles son las dimensiones del dataset (filas, columnas)? Adjunte código o indique cómo determinó la cantidad de datos total. 

```{r}
dim(data_tf)
```
  R: El dataset posee 328 filas y 113 columnas. Hicimos uso de la función dim().
  
2. ¿Qué describe cada línea del dataset? (ejemplifique tomando la fila 85)

```{r}
#Tomamos la fila 85 del dataset
data_tf[85,]
```
  R: Cada linea del dataset corresponde a una instancia, que en este caso corresponden a encuentros locales en ciertas localidades del país. Cada una de esta instancia posee como atributos la localidad donde se realizó el cabildo o encuentro y diferentes conceptos constitucionales, los cuales están contabilizados sobre cuantas veces se mencionaron en cada encuentro.

3. ¿Existen localidades repetidas en el dataset? Adjunte el código o indique cómo llegó a esa conclusión. 

```{r}
dim(unique(data_tf))
```
  R: Utilizamos la función unique() para encontrar los valóres únicos, y con dim(), podemos obtener la cantidad de valores únicos. Se puede notar que el dataset posee 328 filas únicas, que corresponden a la cantidad de filas que posee el dataset. Por lo tanto no existen localidades repetidas.
  
4. Liste los nombres de las columnas del dataset `data_tf`. Adjunte código en R y *recuerde* usar `head` si el resultado es muy largo.

```{r}
#Usamos colnames(), que nos entrega un vector con el nombre de las columnas del dataset
head(colnames(data_tf))
```


### Análisis

1. Liste todas las localidades donde *no* se discutió el concepto `a_la_salud`. 

```{r}
data_tf[data_tf$a_la_salud == 0,]$localidad
```

2. Liste las 10 localidades que más mencionaron el concepto `justicia`. 

```{r}
#Primero ordenamos el vector de justicia de forma decreciente y filtramos el dataset con ese orden, seleccionando las 10 primeras filas/localidades.
data_tf[order(data_tf$justicia, decreasing = TRUE),][1:10,]$localidad
```


3. Liste los 10 conceptos menos mencionados a lo largo de todo el proceso.


```{r}
#Sumamos todas las columnas excepto la primera que no es numérica, y luego ordenamos
names(sort(colSums(data_tf[,-1])))[1:10]

```


4. Liste las 10 localidades que más participaron en el proceso. Describa cómo definió su medida de participación.


```{r}
# Definimos la medida de participacion como la cantidad total registrada de todos los conceptos mencionados en cada localidad
c<-rowSums(data_tf[,-1])
names(c)<-data_tf$localidad
sort(c,decreasing = TRUE)[1:10]

# Enlistamos la suma de todos los conceptos hablados por localidad, los relacionamos con su localidad respectiva, ordenamos decrecientemente y seleccionamos las mejores 10
```

5. Ejecute el siguiente código que permitirá agregar una nueva columna a nuestro dataframe que solo tendrá el nombre de la región.

```{r, message = F, warning=F}
library(dplyr)
regiones <- strsplit(as.character(data_tf[,1]), '/')
data_tf$region <- sapply(regiones, "[[", 1)
data_tf <- data_tf %>% select(localidad, region, everything())
```

Luego, genere un gráfico de barras (ggplot) que muestre los 10 conceptos más mencionados en cada una de las regiones mencionadas (adjunte gráficos y código):

- `Atacama`
- `Coquimbo`
- `Metropolitana de Santiago`


Cabe resaltar, que se esperan tres gráficos de barras para las tres diferentes regiones:

```{r}
atacama <- data_tf %>% filter(region=='Atacama')
atacama$suma <- sum(atacama[,-c(1,2)])
columnas <- colSums(atacama[,-c(1,2)])
atacama_2 <- data.frame(concepto=names(atacama[,-c(1,2)]), suma=columnas)
atacama_2 <- atacama_2 %>% arrange(-suma)
atacama_2 <- atacama_2[1:10,]

library(ggplot2)

ggplot(atacama_2, aes(x = reorder(concepto, suma), y = suma)) +
          geom_bar(stat = "identity") + 
          coord_flip()+
          ggtitle(label = "Top-20 palabras de la colección")


  # 10 conceptos más mencionados en Atacama
```

```{r}
# 10 conceptos más mencionados en Coquimbo
```

```{r}
# 10 conceptos más mencionados en Metropolitana de Santiago
```

6. De la pregunta anterior, ¿considera que es razonable usar el conteo de frecuencias para determinar las regiones que tuvieron mayor participación en el proceso? ¿Por qué? Sugiera y solamente comente una forma distinta de hacerlo.

# RESPUESTA
Antes de comparar la participación en el proceso entre las distintas regiones se debe normalizar cada una de estas a partir de la cantidad de habitantes que posea, puesto que de esta forma se estarían comparando bajo una misma métrica el porcentaje de participación de cada región, por lo que no sería razonable considerar directamente las frecuencias de participación de las regiones para este análisis.


## Ejercicios

### Accidentes de tránsito

Para esta sección utilizaremos un dataset real de número de accidentes de tránsito por localidad, el cual puede ser encontrado en el siguiente link: http://datos.gob.cl/dataset/9348. Para cargar el dataset ejecute el siguiente código:

```{r}
tipos <- read.table("https://users.dcc.uchile.cl/~hsarmien/mineria/datasets/accidentes_2010_2011.txt",encoding = "UTF-8", as.is = FALSE)
head(tipos)
```

Explore el set de datos para responder las siguientes preguntas:

1. Filtre los datos para incluir sólo los accidentes ocurridos el año 2011 a nivel regional. Genere un boxplot donde se indique la cantidad de accidentes categorizado por tipo de accidente.

Este tipo de gráfico nos ayudará a entender como se distribuye los datos por cada tipo de accidentes. Es decir, podremos apreciar que tan dispersos o similares son los datos en todo el dataset. También, puede ser útil para observar valores atípicos u outliers en los datos.

```{r}
#Filtramos por año y regional
filtro_tipos <- tipos[tipos$Muestra=="Regional" & tipos$Anio == 2011,]
#Hacemos el boxplot
plot(filtro_tipos$TipoAccidente, filtro_tipos$Cantidad,xlab = "Tipo de accidente", ylab = "Cantidad", main="Cantidad de accidentes por tipo de accidente en el año 2011")
```

2. ¿Qué aprecia con el gráfico anterior? ¿Qué otra forma de explorar los datos podría agregar? ¿Qué información adicional aporta? Adjunte una breve explicación.

**Respuesta:**

Se puede observar que los accidentes por colisión presentan la mayor variabilidad por región. También se tienen valores outliers que distorsionan el boxplot. Asimismo se puede notar que los accidentes por Caida y Otros presentan muy poca variabilidad y su mediana es menor respecto a los otros tipos de accidentes.

Se podrían realizar histogramas de accidentes por región diferenciados por tipo de accidente, para tener una mejor idea sobre qué tipo de accidente predomina en cada región y tomar medidas pertinentes para esta situación.

También se podría analizar la proporción por tipo de accidentes a nivel de comuna y/o región en comparación a la cantidad de estos a nivel nacional, entonces de esta forma se podría visualizar cuáles son las regiones o comunas que más aportan en porcentaje a las cifras de accidentes en el país. Esto se podría implementar mediante la construcción de gráficos de torta (circular) por tipo de accidente, en los cuales cada trozo del gráfico representaría el porcentaje de ese tipo de accidente en cada región del país, siempre teniendo en consideración que la suma de cada tipo de accidente por región sea igual a la cantidad de ese tipo de accidente a nivel nacional.

### Diamantes

Considere el set de datos diamonds del paquete ggplot2 de R, que contiene los precios en dolares, junto con otros atributos importantes: quilates, corte, color y claridad. También hay medidas físicas como ser: x (largo), y (ancho), z (profundidad), depth (porcentaje total de profundidad) y table (ancho desde el tope del diamante al punto relativo más ancho del diamante):

```{r}
library(ggplot2)
data("diamonds")
head(diamonds)
```

Realice una exploración por el set de datos para responder las siguientes preguntas:

1. Teniendo en cuenta las medidas físicas, ¿considera que existen valores inexistentes o inconsistentes? Describa como manejaría esta situación. Adjunte el código necesario.

```{r}
# RESPUESTA
boxplot(diamonds[,c(1,5,6,8,9,10)], main="Inconsistencias en medidas fisicas")
colSums(is.na(diamonds[,c(1,5,6,8,9,10)]))

```
  R: Dado los resultados expuestos, dentro de las medidas fisicas no existen valores inexistentes (la suma de los na por columna es 0), pero si valores inconsistentes u outliers (Representados en los puntos alejados del boxplot para cada atributo). Para no alterar el comportamiento del set de datos, lo que se podría hacer con los outliers es mantenerlos o modificarlos de tal forma que no afecte las tendencias que tenga el conjunto de datos, esto va a depender de para qué se van a utilizar los datos y qué tan importante es mantener el tamaño del set de datos.
  
2. Considerando la relación entre dos atributos, ¿qué atributos están más correlacionadas con el precio (price) y qué significa esto? ¿cuál es la correlación más alta para table? Adjunte el código necesario para la respuesta.

```{r}
#RESPUESTA
library(corrplot)
correlacion <- cor(diamonds[,-c(2,3,4)])
round(correlacion, digits = 2)
corrplot(correlacion, method = "shade", shade.col = NA, tl.col = "black", tl.srt = 70, addCoef.col = "black")
```
R: Se puede notar que el precio está bastante correlacionado positivamente con las variables carat (quilates), x (largo), y (ancho) y z (profundidad) de los diamantes, pues posee valores muy cercanos a 1, lo cual tiene bastante sentido, ya que un diamante mayores dimensiones tendrá mayor valor.

La variable que está más correlacionada con table es depth, la profundidad del diamante, con un valor absoluto de 0.3, lo que corresponde a una correlación negativa, es decir, que mientras mayor sea la variable table menor será la profundidad del diamante.

3. Proponga otra forma para explorar los datos. ¿Qué información adicional aporta? Adjunte una breve explicación.

**Respuesta:**

Se podrían utilizar medidas de tendencia central para cada atributo de dimensión de los diamantes, tales como el promedio, la mediana y la desviación estándar, esto permitiría entender a priori estas características de los datos.

Si se quisiera ver cuáles son las dimensiones de los diamantes que más se repiten se podría realizar un gráfico (scatter plot) de tres dimensiones con el conjunto de variables c(x,y,z), en donde en aquellos puntos del gráfico que presenten una mayor concentración de datos representarían cuáles son las dimensiones más comunes para este set de datos. Por ejemplo, se podrían observar tendencias sobre las medidas de los diamantes, es decir, podría ocurrir que el largo y el ancho del diamante son directamente proporcionales o lo contrario. 

También se podría realizar un histograma para las variables cualitativas de los diamantes, tales como el color, la claridad y el corte, de esta forma se podría ver cuáles son los parámetros más frecuentes para los diamantes.